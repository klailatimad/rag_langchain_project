# ğŸ§  Local RAG Chat App (LangChain + Chroma + Streamlit)

An experimental Retrieval-Augmented Generation (RAG) chat application built with **LangChain**, **Chroma**, and **Streamlit** â€” running entirely **locally** with support for OpenAI models.

This proof of concept demonstrates how to:
- Build a minimal RAG pipeline with LangChain and OpenAI.
- Store and retrieve chat history using Chroma.
- Run a lightweight Streamlit front end for interactive conversations.

---

## ğŸš€ Current Capabilities

### âœ… RAG Core
- Context-aware retrieval using **Chroma** as a local vector store.
- Uses **OpenAI GPT-3.5-Turbo** for reasoning and answering.
- Simple, reusable chain:  
  - Contextualizes queries using chat history.  
  - Retrieves top-k document chunks.  
  - Generates concise context-aware answers.

### ğŸ’¬ Streamlit Chat UI
- Minimal chat interface for asking and answering questions.
- Persists messages and metadata in Chroma.
- Supports creating, renaming, clearing, and deleting chats.
- Sidebar for switching between saved conversations.

### ğŸ’¾ Local Storage
- All embeddings, metadata, and chat messages stored in ChromaDB locally.
- No cloud dependencies beyond the OpenAI API call.

---

## ğŸ§© Tech Stack

| Layer | Technology | Purpose |
|-------|-------------|----------|
| Front End | **Streamlit** | Interactive local web app |
| LLM | **OpenAI GPT-3.5-Turbo** | Chat & reasoning |
| Embeddings | **OpenAIEmbeddings** | Vector representation of docs |
| Vector Store | **ChromaDB** | Local semantic retrieval |
| Framework | **LangChain** | Chains, retrievers, and prompts |
| Environment | **Python 3.10+**, `venv` | Isolated local setup |

---

## âš™ï¸ Setup & Run

### 1ï¸âƒ£ Clone and create environment
```bash
git clone <your-repo-url>
cd rag_langchain_project
python -m venv .venv
source .venv/bin/activate
```
2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
3ï¸âƒ£ Configure your environment
Create a .env file with your OpenAI key:
```OPENAI_API_KEY=sk-...```
4ï¸âƒ£ Run the app
```bash
streamlit run app_streamlit.py
```
The app will open at http://localhost:8501

ğŸ“ Repository Structure
```
rag_langchain_project/
â”œâ”€â”€ app_streamlit.py    # Streamlit frontend (chat UI + RAG logic)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ chroma_db/          # Local Chroma vector storage (ignored in git)
```

ğŸ§­ Next Steps (Planned Roadmap)
|Priority|	Feature|	Description|
|---|---|---|
|ğŸ”¥ High|	Persistent chat storage (SQLite/Postgres)|Chats and messages survive app restarts|
|ğŸ”¥ High|	Document ingestion (PDFs, text, markdown)|	Upload, chunk, and embed real documents|
|ğŸ§© Medium|	Retrieval controls|	Adjust k, chunk size, overlap, debug panel|
|ğŸ§© Medium|	Streaming responses	|Real-time token streaming in UI|
|ğŸ§© Medium	|Separation of front-end & back-end	|Streamlit â†’ FastAPI + REST endpoints|
|ğŸ§ª Low|	Evaluation harness	|Measure latency, accuracy, token usage|
|ğŸ§± Low|Auth & multi-user|	Separate users and their vector stores|
|âš™ï¸ Low	|LangChain 1.x migration	|Move to new langchain-chroma and langchain-core APIs|

ğŸ§  Key Concepts
- RAG (Retrieval-Augmented Generation): Enhances an LLM by retrieving relevant text chunks and injecting them into the prompt context.
- Chroma: A lightweight, open-source vector database for storing and searching embeddings locally.
- LangChain: A modular framework for chaining together LLMs, retrievers, and prompts into robust applications.